---
title: paper notes - RDMA arch for AI training
date: 2024-09-16 14:34:38
tags:
---

# paper notes - RDMA arch for AI training

了解下当前 RDMA 在 AI training 场景下是怎么用的，这里主要参考了两篇论文：

1. RDMA over Ethernet for Distributed AI Training at Meta Scale
2. Alibaba HPN: A Data Center Network for Large Language Model Training



# Meta

## 1 Background

AI 分布式训练，对数据中心网络基础设施施加了很大的压力。例如，一个典型的生成式AI任务可能需要在数周内紧密协调数千个GPU。构建一个能够满足这种日益增长需求的可靠、高性能网络基础设施，需要对数据中心网络设计进行重新评估。

分布式训练工作负载中的 inter-GPU communication 间通信通常包括两个阶段。在每个训练节点中，包含4-8个GPU，这些GPU通过高速传输技术（如NVLink[26]）相互连接，通常称为“intra-node communication”。当训练任务需要额外的GPU时，“**inter-node communication**”将通过网络进行。业界通常采用两种设计方法来处理这种节点间通信。

一种方法是使用标准的TCP/IP或修改后的套接字实现（例如，fastsocket[7]）。然而，**这种方法容易受到性能下降的影响，因为CPU开销和延迟增加**。第二种设计方法涉及专有互连技术，如InfiniBand[23]、NVSwitch[26]、弹性结构适配器[2]和机架间ICI[8]。尽管**这些方法显著提高了性能，但它们的专有性质限制了其部署的灵活性**。

## 2 Hardware

### **硬件之节点配置**：

随着训练模型和数据集规模的不断增加，将训练过程限制在单个GPU中已变得不可行。这也需要更多的计算和内存，对网络提出了更高的要求，因此需要一个专门设计的扩展系统。

![image-20240916204256020](../figures/image-20240916204256020.png)

第一代训练节点设计，ZionEX[20]，结合了通用CPU和NVIDIA A100 GPU。更新的代际，**Grand Teton**[17]，基于H100 GPU。ZionEX和Grand Teton都采用了类似的系统架构，除了GPU NVLink互连。图4展示了Grand Teton的内部结构。**节点分为3个托盘(tray)**

1. CPU托盘(tray)，包含2个CPU和前端NIC (tray)
2. 交换机托盘，包含4个PCIe Gen5交换机、NVMe存储以及8个RDMA NIC
3. GPU tray，包含8个GPU。GPU通过NVSwitch[26]完全连接。

GPU和NIC之间存在1:1的映射。对于使用少于8个GPU的训练任务，GPU之间的通信在节点内进行，无需网络操作。对于更大的任务，RDMA NIC启用GPUDirect技术，使得GPU到GPU的流量可以绕过主机和主机内存瓶颈。

### **硬件之网络配置**：

训练集群依赖于两个独立的网络：前端网络（FE）用于数据摄取、检查点和日志记录等任务，后端网络（BE）用于训练，如图5所示。

![image-20240916204834837](../figures/image-20240916204834837.png)

前端网络：一个训练机架连接到数据中心网络的FE和BE。FE具有网络层次结构[1]——机架交换机（RSW）、结构交换机（FSW）和更高层——这些层包含存储仓库，为GPU提供训练工作负载所需的输入数据。我们确保机架交换机上有足够的入口带宽，以不阻碍训练工作负载。

![image-20240916204935978](../figures/image-20240916204935978.png)

后端网络：如图六所示，BE是一个专门设计的结构，以非阻塞架构连接所有RDMA NIC，为集群中任意两个GPU之间提供高带宽、低延迟和无损传输，无论它们的物理位置如何。这个后端结构使用RoCEv2协议，该协议将RDMA服务封装在UDP数据包中，以便通过网络传输。



# Alibaba HPN

## 1 架构说明

![image-20240916210344963](../figures/image-20240916210344963.png)

HPN由前端和后端网络组成。**后端网络主要支持训练过程中的流量，而前端网络承载其他流量（例如，管理、推理和存储的流量）**。对于大语言模型（LLM）训练，我们主要关注HPN的后端网络。在HPN的后端网络中，每台主机配备8个GPU，每个GPU通过专用的高带宽主机内网络（例如，NVLINK [49]）连接。每个GPU可以通过这个主机内网络以400GBps-900GBps（双向）的速度直接与其他GPU通信。

为了提供最大的网络容量，我们为每台主机配备了9个NIC，每个NIC为2×200Gbps。这九个NIC中的一个（即图7中的NIC0）连接到前端网络，而其余八个NIC连接到后端网络，以在LLM训练期间承载流量。这八个NIC中的每一个都服务于一个专用的GPU（称为轨道），因此每个GPU都有专用的400Gbps RDMA网络吞吐量，总带宽为3.2Tbps。这种设计旨在最大化GPU的PCIe能力（PCIe Gen5×16），从而将网络发送/接收能力推向极限。每个NIC的两个端口分别连接到不同的ToR，形成一个双ToR设计。这种双ToR设计旨在避免单ToR故障问题，这是§4中详细介绍的关键设计。

在第一层（Tier1），由于主机内网络（例如，NVLINK）的容量高于主机间以太网网络，我们采用了轨道优化设计[21]，其中属于不同轨道的NIC连接到不同的ToR集合。结合上述提到的双ToR设计，一台主机在后端网络中连接到16个ToR。通过充分利用51.2Tbps交换芯片的能力，HPN使得1024个GPU通过单层网络互联，称为一个段（§5）。图6量化了我们的收益：大约96.3%的生产中的大语言模型（LLM）训练任务使用的GPU数量少于1000个；因此，在HPN中，这些任务可以放在一个段中，实现最大的网络性能。

第二层（Tier2）连接多个段。我们结合了双ToR特性，在这一层设计了双平面转发（如图7所示）。从源NIC的端口0发送的流量可以通过网络转发，并最终仅由目标NIC的端口0接收，与源NIC端口1的流量物理隔离。这种双平面设计避免了聚合层的哈希极化问题，同时不影响1:1的网络二分带宽。此外，双平面设计将一个Pod覆盖的GPU数量翻倍，支持15K个GPU的互联（§6）。

对于未来单个任务可能需要更大规模的情况，我们还设计了不同Pod之间的核心层互联。由于单个Pod的规模已经达到15K个GPU，需要通过核心层进行协调的任务很少。在我们的设计中，我们在聚合-核心层选择了15:1的超额订阅。根据LLM训练的流量特性，我们分配了跨Pod的PP通信，确保跨Pod传输对端到端训练性能的影响最小（§7）。

**对于前端，它主要用于承载管理、推理和存储的流量。前端和后端网络的物理分离确保了前端流量不会影响训练任务的主要过程。**此外，**前端网络设计为1:1的超额订阅，使其可扩展到更多场景，例如LLM训练和推理的混合部署（§8）**。

## 2 独立的前端网络

在HPN中，每台主机配备一个额外的2×200Gbps NIC用于前端网络访问，我们设计了一个独立的前端网络。前端网络主要处理管理和存储流量（例如，**集群管理、数据集加载、镜像加载和检查点保存/加载**）。**它还可以在提供模型推理服务时承载推理请求/响应。**由于前端网络支持的流量与传统云计算场景相似，如图7所示，我们为前端网络采用了经典的3层拓扑结构。为了确保可靠性，每个前端NIC以非堆叠的双ToR方式连接到两个ToR。在前端网络中，我们将聚合层和核心层的收敛比设计为1:1，以保证最大的二分带宽。

**将存储流量与训练隔离**：为了提供最佳的训练性能，存储集群位于前端网络中（关于存储集群位置的详细讨论见§10）。一个存储集群由96-128个存储主机组成，运行CPFS和OSS存储服务，以存储训练数据集、训练所需的容器镜像以及训练过程中保存的检查点。

**支持推理兼容性**：在过去几年中，训练和推理的主流GPU有所不同。例如，NVIDIA V100、A100和H100是为训练设计的，而NVIDIA T4、A10和H10主要用于推理。然而，有一种趋势是使用训练GPU进行推理。原因有两点：（1）随着模型规模的增加，推理服务需要更高内存和性能的GPU，用于推理的GPU规格越来越接近用于训练的GPU。（2）我们观察到许多客户更喜欢在同一租用的集群上部署训练和推理任务，以提高GPU利用率。

在设计HPN前端网络时，我们充分考虑了上述需求，2×200Gbps前端网络提供了良好的性能。因此，这种设计确保了HPN中的主机可以灵活地用于训练和推理，构建一个支持用户各种需求的统一平台。
